{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Counterfactual Explanations:\n",
        "\n",
        "In this notebook, we will explore counterfactual explanations, which are post-hoc explainability methods specifying parts of an image ."
      ],
      "metadata": {
        "id": "HOulXVTnbUdq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF4n7mJCbRdg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from glob import glob\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from imgaug import augmenters as iaa\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.modules import Module\n",
        "# from utils.utils import get_roc_auc_score\n",
        "# from utils.utils import prototype_heatmap\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, recall_score, precision_score\n",
        "from torchvision import transforms\n",
        "import sys\n",
        "import argparse\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from datetime import datetime\n",
        "from model_intepretability.imaging_copy.dataset.dataset import PrototypicalBatchSampler\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from functools import partial\n",
        "import torch.distributed as dist\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, recall_score, precision_score\n",
        "from torch.utils.data import DataLoader,SubsetRandomSampler, TensorDataset\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.models as models\n",
        "import torch.distributed as dist\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Variable\n",
        "plt.rcParams['figure.figsize'] = [25, 10]\n",
        "import gradio as gr\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import gc\n",
        "from pathlib import Path\n",
        "# from utils.utils import get_roc_auc_score\n",
        "from omnixai.data.image import Image\n",
        "from omnixai.explainers.vision import CounterfactualExplainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gather(tensor, tensor_list=None, root=0, group=None):\n",
        "    \"\"\"\n",
        "        Sends tensor to root process, which store it in tensor_list.\n",
        "    \"\"\"\n",
        "\n",
        "    rank = dist.get_rank()\n",
        "    if group is None:\n",
        "        group = dist.group.WORLD\n",
        "    if rank == root:\n",
        "        assert(tensor_list is not None)\n",
        "        dist.gather(tensor, gather_list=tensor_list, group=group)\n",
        "    else:\n",
        "        dist.gather(tensor, dst=root, group=group)\n",
        "\n",
        "\n",
        "\n",
        "def compute_test_auc(model, test_loader, device_id):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.cuda(device_id, non_blocking=True), labels.cuda(device_id, non_blocking=True)\n",
        "            outputs,_ = model(images)\n",
        "            preds = torch.sigmoid(outputs)#.squeeze()  # Assuming binary classification\n",
        "\n",
        "            # Gather predictions and labels across all processes\n",
        "\n",
        "            gathered_preds = [torch.zeros_like(preds) for _ in range(dist.get_world_size())]\n",
        "            gathered_labels = [torch.zeros_like(labels) for _ in range(dist.get_world_size())]\n",
        "\n",
        "            # Collect predictions and labels from each process\n",
        "\n",
        "            dist.all_gather(gathered_preds,preds)\n",
        "            dist.all_gather(gathered_labels,labels)\n",
        "\n",
        "\n",
        "            # Only concatenate and store predictions and labels on rank 0\n",
        "            if dist.get_rank() == 0:\n",
        "                all_preds.extend([p.cpu() for p in gathered_preds])\n",
        "                all_labels.extend([l.cpu() for l in gathered_labels])\n",
        "\n",
        "\n",
        "    # On rank 0, compute AUC\n",
        "    if dist.get_rank() == 0:\n",
        "        all_preds = torch.cat(all_preds)\n",
        "        all_labels = torch.cat(all_labels)\n",
        "        print(\"lllll\",all_labels.shape)\n",
        "        print(\"sssss\",all_preds.shape)\n",
        "        auc = get_roc_auc_score(all_labels.numpy(), all_preds.numpy())\n",
        "        print(f\"AUC: {auc}\")\n",
        "        return auc\n",
        "\n",
        "def aggregate_all_predictions(predictions, labels,device_id):\n",
        "\n",
        "\n",
        "    # Initialize lists to gather predictions and labels from all devices\n",
        "\n",
        "    predictions = torch.tensor(predictions).cuda(device_id)\n",
        "    labels = torch.tensor(labels).cuda(device_id)\n",
        "\n",
        "    gathered_preds = [torch.zeros_like(predictions) for _ in range(dist.get_world_size())]\n",
        "    gathered_labels = [torch.zeros_like(labels) for _ in range(dist.get_world_size())]\n",
        "\n",
        "    # Use all_gather to collect all data across devices\n",
        "    dist.all_gather(gathered_preds, predictions)\n",
        "    dist.all_gather(gathered_labels, labels)\n",
        "\n",
        "    # Concatenate data across devices\n",
        "    all_preds = torch.cat(gathered_preds)\n",
        "    all_labels =torch.cat(gathered_labels)\n",
        "\n",
        "    # Convert to numpy for AUC calculation\n",
        "    return all_preds.cpu().numpy(), all_labels.cpu().numpy()\n",
        "\n",
        "def load_checkpoint(model, optimizer,file_path):\n",
        "    checkpoint = torch.load(file_path)\n",
        "\n",
        "    checkpoint_state_dict = checkpoint['model_state_dict']\n",
        "\n",
        "\n",
        "    # Create a new state_dict with only the matching keys\n",
        "    checkpoint_state_dict = {k.replace('module.', ''): v for k, v in checkpoint_state_dict.items()}\n",
        "\n",
        "    filtered_state_dict = {k: v for k, v in checkpoint_state_dict.items() if k in model.state_dict()}# and \"mlp.mlp_layers.dense_1\" not in k and \"mlp.mlp_layers.dense_0\" not in k and \"num_norm\" not in k}\n",
        "\n",
        "    model.load_state_dict(filtered_state_dict, strict=False)\n",
        "\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    return model, optimizer, epoch, loss\n",
        "\n",
        "def worker_init_fn(worker_id: int, num_workers: int, rank: int, seed: int) -> None:\n",
        "    \"\"\"Initialize worker processes with a random seed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    worker_id : int\n",
        "        ID of the worker process.\n",
        "    num_workers : int\n",
        "        Total number of workers that will be initialized.\n",
        "    rank : int\n",
        "        The rank of the current process.\n",
        "    seed : int\n",
        "        A random seed used determine the worker seed.\n",
        "    \"\"\"\n",
        "    worker_seed = num_workers * rank + worker_id + seed\n",
        "    torch.manual_seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss,file_path):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }\n",
        "    torch.save(checkpoint, file_path)\n",
        "\n",
        "def setup() -> None:\n",
        "    \"\"\"Initialize the process group.\"\"\"\n",
        "    dist.init_process_group(\"nccl\")#,rank=dist.get_rank(), init_method='env://')\n",
        "\n",
        "\n",
        "def cleanup() -> None:\n",
        "    \"\"\"Clean up the process group after training.\"\"\"\n",
        "    dist.destroy_process_group()"
      ],
      "metadata": {
        "id": "mBhzQO1kgQln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_workers=4\n",
        "resize=224\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "scheduler = False\n",
        "step_size = 5\n",
        "\n",
        "dropout = 0.5\n",
        "rseed = 42\n",
        "weight_decay = 1e-3\n",
        "num_epochs = 20\n",
        "its = 100\n",
        "cTr = 15\n",
        "nsTr = 5\n",
        "nqTr = 5\n",
        "cVa = 5\n",
        "nsVa = 5\n",
        "nqVa = 15\n"
      ],
      "metadata": {
        "id": "Qr6fJn2ZekPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data:\n",
        "\n",
        "In this project, we use the \"NIH\" dataset, which is publicly accessible at [here](https://www.kaggle.com/datasets/nih-chest-xrays/data). It contains 112120 chest X-rays corresponding to 14 different conditions, e.g., Pneumonia. The goal is to train a predictive model to classify each image to one or multiple conditions."
      ],
      "metadata": {
        "id": "gJ-Y69Kjf06v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = image/np.max(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "class XrayDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_path_name):\n",
        "\n",
        "        self.path_name = image_path_name\n",
        "        self.csv_file = csv_file\n",
        "        self.the_chosen, self.all_classes, self.all_classes_dict = self.choose_the_indices()\n",
        "        self.csv_file[\"numeric_targets\"] = self.csv_file['Finding Labels'].apply(lambda x: self.get_tagets(x))\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.csv_file)\n",
        "    def choose_the_indices(self):\n",
        "\n",
        "        max_examples_per_class = 10000 # its the maximum number of examples that would be sampled in the training set for any class\n",
        "        the_chosen = []\n",
        "        all_classes = {}\n",
        "        length = len(self.csv_file)\n",
        "        print('\\nSampling the huuuge training dataset')\n",
        "        for i in tqdm(list(np.random.choice(range(length),length, replace = False))):\n",
        "\n",
        "            temp = str.split(self.csv_file.iloc[i, :]['Finding Labels'], '|')\n",
        "\n",
        "            # special case of ultra minority hernia. we will use all the images with 'Hernia' tagged in them.\n",
        "            if 'Hernia' in temp:\n",
        "                the_chosen.append(i)\n",
        "                for t in temp:\n",
        "                    if t not in all_classes:\n",
        "                        all_classes[t] = 1\n",
        "                    else:\n",
        "                        all_classes[t] += 1\n",
        "                continue\n",
        "\n",
        "            # choose if multiple labels\n",
        "            if len(temp) > 1:\n",
        "                bool_lis = [False]*len(temp)\n",
        "                # check if any label crosses the upper limit\n",
        "                for idx, t in enumerate(temp):\n",
        "                    if t in all_classes:\n",
        "                        if all_classes[t]< max_examples_per_class: # 500\n",
        "                            bool_lis[idx] = True\n",
        "                    else:\n",
        "                        bool_lis[idx] = True\n",
        "                # if all lables under upper limit, append\n",
        "                if sum(bool_lis) == len(temp):\n",
        "                    the_chosen.append(i)\n",
        "                    # maintain count\n",
        "                    for t in temp:\n",
        "                        if t not in all_classes:\n",
        "                            all_classes[t] = 1\n",
        "                        else:\n",
        "                            all_classes[t] += 1\n",
        "            else:        # these are single label images\n",
        "                for t in temp:\n",
        "                    if t not in all_classes:\n",
        "                        all_classes[t] = 1\n",
        "                    else:\n",
        "                        if all_classes[t] < max_examples_per_class: # 500\n",
        "                            all_classes[t] += 1\n",
        "                            the_chosen.append(i)\n",
        "\n",
        "\n",
        "\n",
        "        '''\n",
        "        if len(the_chosen) != len(set(the_chosen)):\n",
        "            print('\\nGadbad !!!')\n",
        "            print('and the difference is: ', len(the_chosen) - len(set(the_chosen)))\n",
        "        else:\n",
        "            print('\\nGood')\n",
        "        '''\n",
        "        with open('all_classes.pkl', 'wb') as file:\n",
        "            pickle.dump(all_classes, file)\n",
        "        return the_chosen, sorted(list(all_classes)), all_classes\n",
        "\n",
        "    def get_tagets(self,row):\n",
        "        labels = str.split(row, '|')\n",
        "\n",
        "        target = torch.zeros(len(self.all_classes))\n",
        "        for lab in labels:\n",
        "            lab_idx = self.all_classes.index(lab)\n",
        "            target[lab_idx] = 1\n",
        "        return target\n",
        "    def get_image(self, idx):\n",
        "        # -- Query the index location of the required file\n",
        "\n",
        "        image_name = self.csv_file.loc[idx,'Image Index']\n",
        "\n",
        "        image_path = glob(os.path.join(self.path_name, '**', image_name), recursive=True)[0]\n",
        "        image = read_image(image_path)\n",
        "        if len(image.shape) == 2: image = np.expand_dims(image, axis=-1)\n",
        "\n",
        "        labels = str.split(self.csv_file.loc[idx,'Finding Labels'], '|')\n",
        "\n",
        "        target = torch.zeros(len(self.all_classes))\n",
        "        for lab in labels:\n",
        "            lab_idx = self.all_classes.index(lab)\n",
        "            target[lab_idx] = 1\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "        input_size = 224\n",
        "        rseed = 42\n",
        "        seq = iaa.Sequential([iaa.Resize((input_size, input_size))])\n",
        "        image_transform = transforms.Compose([seq.augment_image, transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        image = image_transform(image)\n",
        "        return image.float(), target\n",
        "\n",
        "\n",
        "    def num_sort(self, filename):\n",
        "        not_num = re.compile(\"\\D\")\n",
        "        return int(not_num.sub(\"\", filename))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.get_image(idx)\n",
        "        return image,label\n"
      ],
      "metadata": {
        "id": "RPHiQ_fyeovk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model architechture\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
        "        self.key = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
        "        self.value = nn.Conv2d(in_dim, in_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, width, height = x.size()\n",
        "        query = self.query(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
        "        key = self.key(x).view(batch_size, -1, width * height)\n",
        "        # calculating attention weights\n",
        "        attention = torch.bmm(query, key)\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        value = self.value(x).view(batch_size, -1, width * height)\n",
        "        # calculating weighted context\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, width, height)\n",
        "        out = self.gamma * out + x # weighted context added to the original input in a residual manner\n",
        "        return out, attention\n",
        "\n",
        "class ResNetAttention(nn.Module):\n",
        "    def __init__(self, original_model,num_classes=15):\n",
        "        super(ResNetAttention, self).__init__()\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-2])\n",
        "        self.attention = SelfAttention(in_dim=512)  # Adjust channels based on ResNet block\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(512, 256)  # Output channels should match in_channels of attention layer\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x, attention = self.attention(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x, attention"
      ],
      "metadata": {
        "id": "dsycFltgeoyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fo7FuQoDeo1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "go9mNumJeo4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, recall_score, precision_score\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "def get_roc_auc_score(y_true, y_probs):\n",
        "    '''\n",
        "    Uses roc_auc_score function from sklearn.metrics to calculate the micro ROC AUC score for a given y_true and y_probs.\n",
        "    '''\n",
        "\n",
        "    with open('all_classes.pkl', 'rb') as all_classes:\n",
        "        all_classes = pickle.load(all_classes)\n",
        "\n",
        "    NoFindingIndex = all_classes.get('No Finding', -1)\n",
        "    class_roc_auc_list = []\n",
        "    useful_classes_roc_auc_list = []\n",
        "\n",
        "    for i in range(y_true.shape[1]):\n",
        "        if len(np.unique(y_true[:, i])) > 1:\n",
        "            class_roc_auc = roc_auc_score(y_true[:, i], y_probs[:, i])\n",
        "            class_roc_auc_list.append(class_roc_auc)\n",
        "            if i != NoFindingIndex:\n",
        "                useful_classes_roc_auc_list.append(class_roc_auc)\n",
        "    return np.mean(np.array(useful_classes_roc_auc_list))\n",
        "\n"
      ],
      "metadata": {
        "id": "5wqGEbaveo8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "htNF_9kBf_dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "auc_list , precision_list, recall_list, f1_list = [], [], [], []\n",
        "random.seed(rseed)\n",
        "np.random.seed(rseed)\n",
        "torch.manual_seed(rseed)\n",
        "torch.cuda.manual_seed(rseed)\n",
        "torch.cuda.manual_seed_all(rseed)\n",
        "\n",
        "image_path = \"/datasets/nih-chest-xrays\"\n",
        "\n",
        "csv_file = pd.read_csv(os.path.join(image_path,\"Data_Entry_2017.csv\"))\n",
        "test_split = os.path.join(image_path,\"test_list.txt\")\n",
        "train_val_split = os.path.join(image_path,\"train_val_list.txt\")\n",
        "with open(train_val_split, 'r') as f:\n",
        "    train_val_images = f.read().splitlines()\n",
        "with open(test_split, 'r') as f:\n",
        "    test_images = f.read().splitlines()\n",
        "\n",
        "train_df = csv_file[csv_file['Image Index'].isin(train_val_images)]\n",
        "\n",
        "test_df = csv_file[csv_file['Image Index'].isin(test_images)]\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "train_ds = XrayDataset(train_df, image_path)\n",
        "test_ds = XrayDataset(test_df, image_path)\n",
        "train_prototype(args, train_ds,test_ds)"
      ],
      "metadata": {
        "id": "wHo_8r0rgBak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracted from: https://captum.ai/tutorials/CIFAR_Captum_Robustness\n",
        "classes = ('0','1','2','3','4','5','6','7','8','9','10','11','12','13','14')\n",
        "def image_show(img, pred):\n",
        "    npimg = img.squeeze().permute(1, 2, 0).cpu().detach().numpy()\n",
        "    plt.imshow(npimg)\n",
        "    plt.title(\"prediction: %s\" % pred)\n",
        "    plt.show()\n",
        "    plt.savefig(\"counter_plot.png\")\n",
        "\n",
        "\n",
        "def get_prediction(model, input, normalize_im=False):\n",
        "    if normalize:\n",
        "      output = model(input)\n",
        "      _, pred = torch.max(output, dim=1)\n",
        "    return classes[pred], torch.sigmoid(output)"
      ],
      "metadata": {
        "id": "Y3teDi9XuCUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracted from: https://captum.ai/tutorials/CIFAR_Captum_Robustness\n",
        "def counterfactual_explanation(model, image,label):\n",
        "    image = image.unsqueeze(0)\n",
        "    normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    image.requires_grad = True\n",
        "    # Get original prediction\n",
        "    pred, score  = get_prediction(model, image, normalize_im=False)\n",
        "    preprocess = lambda ims: ims.to(image.device)\n",
        "\n",
        "    feature_mask = torch.arange(64).reshape(8,8).repeat_interleave(repeats=28, dim=1).repeat_interleave(repeats=28, dim=0).reshape(1,1,224,224).to(image.device)\n",
        "    ablator = FeatureAblation(model)\n",
        "\n",
        "    active_labels = torch.nonzero(label.squeeze(), as_tuple=True)[0].to(image.device) # Get the indices of the active labels\n",
        "\n",
        "\n",
        "    attr = ablator.attribute(image, target=active_labels.to(image.device), feature_mask=feature_mask.to(image.device))\n",
        "    # Choose single channel, all channels have same attribution scores\n",
        "    pixel_attr = attr[:,0:1].to(image.device)\n",
        "    def pixel_dropout(image, dropout_pixels):\n",
        "\n",
        "        keep_pixels = image[0][0].numel() - int(dropout_pixels)\n",
        "        vals, _ = torch.kthvalue(pixel_attr.flatten(), keep_pixels)\n",
        "        return (pixel_attr < vals.item()) * image\n",
        "\n",
        "    min_pert_attr = MinParamPerturbation(forward_func=model.to(image.device), attack=pixel_dropout, arg_name=\"dropout_pixels\", mode=\"linear\",\n",
        "                                     arg_min=0, arg_max=1024, arg_step=16,\n",
        "                                     preproc_fn=preprocess, apply_before_preproc=True)\n",
        "    pixel_dropout_im, pixels_dropped = min_pert_attr.evaluate(image, target=active_labels.to(image.device), perturbations_per_eval=10)\n",
        "    # Feature Dropout Image\n",
        "    new_pred_dropout, score_dropout = get_prediction(model, pixel_dropout_im, normalize_im=False)\n",
        "    score_dropout_values = \", \".join(map(str, score_dropout.tolist()))\n",
        "    image_show(pixel_dropout_im, new_pred_dropout)"
      ],
      "metadata": {
        "id": "SMDwvXI3VJgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_baseline(args, train_ds,test_ds):\n",
        "\n",
        "\n",
        "\n",
        "    batch_size=args.batch_size\n",
        "\n",
        "\n",
        "    classifier_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    global f1_list,auc_list, precision_list, recall_list\n",
        "    setup()\n",
        "    init_fn = partial(\n",
        "        worker_init_fn,\n",
        "        num_workers=args.num_workers,\n",
        "        rank=dist.get_rank(),\n",
        "        seed=args.rseed,\n",
        "    )\n",
        "\n",
        "    test_sampler = DistributedSampler(test_ds, shuffle=True)\n",
        "\n",
        "    test_dl = DataLoader(test_ds,sampler=test_sampler, batch_size=args.batch_size,worker_init_fn=init_fn)\n",
        "\n",
        "    # setup()\n",
        "    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
        "    torch.cuda.empty_cache()\n",
        "    device_id = torch.cuda.current_device()\n",
        "\n",
        "\n",
        "\n",
        "    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_ds)))):\n",
        "\n",
        "        print(\"fold:\", fold)\n",
        "\n",
        "        print('Fold {}'.format(fold + 1))\n",
        "\n",
        "        train_subset = Subset(train_ds, train_idx)\n",
        "        val_subset = Subset(train_ds, val_idx)\n",
        "        train_sampler = DistributedSampler(train_subset, shuffle=True)\n",
        "        valid_sampler = DistributedSampler(val_subset, shuffle=True)\n",
        "        init_fn = partial(worker_init_fn,num_workers=args.num_workers,rank=dist.get_rank(),seed=args.rseed)\n",
        "        train_dl = DataLoader(train_ds, sampler=train_sampler,batch_size=args.batch_size,worker_init_fn=init_fn,pin_memory=False,drop_last=True,num_workers=args.num_workers)\n",
        "        valid_dl = DataLoader(train_ds, sampler=valid_sampler, batch_size=args.batch_size,worker_init_fn=init_fn,pin_memory=False,drop_last=True,num_workers=args.num_workers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        model2 = ResNetAttention(models.resnet18(pretrained=True))\n",
        "        model2 = model2.cuda(device_id)\n",
        "        model2 = DDP(model2, device_ids=[device_id])#,find_unused_parameters=True)\n",
        "        optimizer = optim.AdamW(model2.parameters(), lr=args.lr,weight_decay=args.weight_decay)\n",
        "\n",
        "        try:\n",
        "            model2, optimizer, start_epoch, _ = load_checkpoint(model2, optimizer,\"output_weight/resnet.pth\")\n",
        "            print(f\"Resuming from epoch {start_epoch + 1}\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"No checkpoint found, starting frsom scratch\")\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.995, step_size=1)\n",
        "        optimizer.zero_grad()\n",
        "        cl_list=[]\n",
        "\n",
        "\n",
        "        for epoch in range(args.num_epochs):\n",
        "            if epoch<=20:\n",
        "                continue\n",
        "            model2.train()\n",
        "            train_loss = 0\n",
        "            counter = 0\n",
        "            num_batches=0\n",
        "            val_batch=0\n",
        "            train_batches=0\n",
        "            epoch_loss = 0\n",
        "\n",
        "            train_loader_examples_num = len(train_dl.dataset)\n",
        "            training_estimated = np.zeros((train_loader_examples_num, 15), dtype = np.float32)\n",
        "            training_ture  = np.zeros((train_loader_examples_num, 15), dtype = np.float32)\n",
        "            k=0\n",
        "            for num_batches,(images,labels) in enumerate(train_dl):\n",
        "\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                images = images.cuda(device_id, non_blocking=True)\n",
        "                counterfactual_explanation(model2, images[0],labels[0])\n",
        "                labels = labels.cuda(device_id, non_blocking=True)\n",
        "\n",
        "\n",
        "                logits= model2(images.float())\n",
        "                prob = torch.sigmoid(logits)\n",
        "\n",
        "                loss = classifier_criterion(logits, labels)\n",
        "                heatmap = gradcam(model2,images,pred)\n",
        "                model2.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "\n",
        "\n",
        "                optimizer.step()\n",
        "                if (num_batches+1)%8==0:\n",
        "                    save_checkpoint(model2, optimizer, epoch, loss.item(), file_path=f\"output_weight/resnet_backup.pth\")\n",
        "\n",
        "                save_checkpoint(model2, optimizer, epoch, loss.item(), file_path=f\"output_weight/resnet.pth\")\n",
        "\n",
        "                training_estimated[k: k + prob.shape[0], :] = prob.detach().cpu().numpy()\n",
        "                training_ture[k: k + prob.shape[0], :] = labels.detach().cpu().numpy()\n",
        "                k += prob.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            model2.eval()\n",
        "\n",
        "            with torch.set_grad_enabled(False):\n",
        "\n",
        "                val_loss = 0.0\n",
        "                val_b=0\n",
        "                total_epoch = 0\n",
        "\n",
        "                val_loader_examples_num = len(valid_dl.dataset)\n",
        "                validation_estimated = np.zeros((val_loader_examples_num, 15), dtype = np.float32)\n",
        "                validation_true  = np.zeros((val_loader_examples_num, 15), dtype = np.float32)\n",
        "                n = 0\n",
        "\n",
        "                k = 0\n",
        "                for val_batches,(images,labels) in enumerate(valid_dl):\n",
        "\n",
        "                    labels =  labels.cuda(device_id, non_blocking=True)\n",
        "\n",
        "\n",
        "                    images = images.cuda(device_id, non_blocking=True)\n",
        "                    logits,attention_weights = model2(images)\n",
        "\n",
        "\n",
        "\n",
        "                    prob = torch.sigmoid(logits)\n",
        "\n",
        "                    validation_estimated[k: k + prob.shape[0], :] = prob.detach().cpu().numpy()\n",
        "                    validation_true[k: k + prob.shape[0], :] = labels.detach().cpu().numpy()\n",
        "                    k += prob.shape[0]\n",
        "\n",
        "            val_auc = get_roc_auc_score(validation_true, validation_estimated)\n",
        "\n",
        "            train_auc = get_roc_auc_score(training_ture, training_estimated)\n",
        "\n",
        "\n",
        "\n",
        "            print(\"epoch\",epoch,\":\",\"train_AUC:\",train_auc,\"val_AUC\",val_auc)\n",
        "            model2.eval()\n",
        "        with torch.set_grad_enabled(False):\n",
        "\n",
        "\n",
        "            test_loader_examples_num = len(test_dl.dataset)\n",
        "            test_estimated = np.zeros((test_loader_examples_num, 15), dtype = np.float32)\n",
        "            test_true  = np.zeros((test_loader_examples_num, 15), dtype = np.float32)\n",
        "            n = 0\n",
        "\n",
        "            k = 0\n",
        "\n",
        "            all_predictions, all_labels = [], []\n",
        "            for images,labels in test_dl:\n",
        "\n",
        "\n",
        "                images, labels = images.cuda(device_id, non_blocking=True), labels.cuda(device_id, non_blocking=True)\n",
        "\n",
        "\n",
        "\n",
        "                logits,attention_weights = model2(images)\n",
        "\n",
        "                logits = torch.squeeze(logits)\n",
        "\n",
        "                prob = torch.sigmoid(logits)\n",
        "\n",
        "                threshold = 0.5\n",
        "                pred =(prob >= threshold).float()\n",
        "                print(\"this shape\",attention_weights.shape,images.shape,pred.shape, attention_weights[0].shape,images[0].shape,pred[0].shape)\n",
        "                visualize_trainable_attention(attention_weights[0],images[0],labels[0],pred[0])\n",
        "\n",
        "                test_estimated[k: k + prob.shape[0], :] = prob.detach().cpu().numpy()\n",
        "                test_true[k: k + prob.shape[0], :] = labels.detach().cpu().numpy()\n",
        "\n",
        "                k += prob.shape[0]\n",
        "\n",
        "                all_predictions = test_estimated\n",
        "                all_labels = test_true\n",
        "\n",
        "\n",
        "            final_preds, final_labels = aggregate_all_predictions(all_predictions, all_labels,device_id)\n",
        "\n",
        "            # Calculate AUC on rank 0\n",
        "            if dist.get_rank() == 0:\n",
        "                auc = get_roc_auc_score( final_labels, final_preds)\n",
        "                print(f\"Test AUC: {auc}\")\n",
        "\n",
        "\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    logging.info('Finished training.')\n",
        "\n",
        "    dist.destroy_process_group()\n",
        "    return 0"
      ],
      "metadata": {
        "id": "qjZPkT0cvFCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "[[1] Captum Robustness with Image Classification](https://captum.ai/tutorials/CIFAR_Captum_Robustness)"
      ],
      "metadata": {
        "id": "MrdDs6c0gKUY"
      }
    }
  ]
}