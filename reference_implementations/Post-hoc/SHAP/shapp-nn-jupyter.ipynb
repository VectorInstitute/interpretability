{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. SHAP for Neural Network Models**\n",
    "\n",
    "SHAP is model-agnostic and can be used to explain other black box models as well, for example, neural networks.\n",
    "The shap module in python has something called KernelExplainer which can be used for NNs.\n",
    "\n",
    "Let's illustrate this using a [Credit Card Defaulter dataset from Taiwan](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients)\n",
    "\n",
    "The dataset has the following 23 variables as explanatory variables:\n",
    "| Variable | Description |\n",
    "|----------|------------|\n",
    "| LIMIT_BAL  | Amount of the given credit (NT dollar): includes both individual consumer credit and family (supplementary) credit. |\n",
    "| SEX  | Gender (1 = male; 2 = female). |\n",
    "| EDUCATION  | Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). |\n",
    "| MARRIAGE  | Marital status (1 = married; 2 = single; 3 = others). |\n",
    "| AGE  | Age (years). |\n",
    "| PAY_0  | Repayment status in September 2005 (-1 = pay duly; 1 = delay for 1 month; 2 = delay for 2 months; ...; 8 = delay for 8 months; 9 = delay for 9+ months). |\n",
    "|  PAY_2 | Repayment status in August 2005. |\n",
    "| PAY_3  | Repayment status in July 2005. |\n",
    "| PAY_4  | Repayment status in June 2005. |\n",
    "| PAY_5 | Repayment status in May 2005. |\n",
    "| PAY_6 | Repayment status in April 2005. |\n",
    "| BILL_AMT1 | Amount of bill statement (NT dollar) in September 2005. |\n",
    "| BILL_AMT2 | Amount of bill statement (NT dollar) in August 2005. |\n",
    "| BILL_AMT3 | Amount of bill statement (NT dollar) in July 2005. |\n",
    "| BILL_AMT4 | Amount of bill statement (NT dollar) in June 2005. |\n",
    "| BILL_AMT5 | Amount of bill statement (NT dollar) in May 2005. |\n",
    "| BILL_AMT6 | Amount of bill statement (NT dollar) in April 2005. |\n",
    "| PAY_AMT1 | Amount of previous payment (NT dollar) in September 2005. |\n",
    "| PAY_AMT2 | Amount of previous payment (NT dollar) in August 2005. |\n",
    "| PAY_AMT3 | Amount of previous payment (NT dollar) in July 2005. |\n",
    "| PAY_AMT4 | Amount of previous payment (NT dollar) in June 2005. |\n",
    "| PAY_AMT5 | Amount of previous payment (NT dollar) in May 2005. |\n",
    "| PAY_AMT6 | Amount of previous payment (NT dollar) in April 2005. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xlrd to read excel files in pandas\n",
    "import pandas as pd \n",
    "import lime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import lime.lime_tabular\n",
    "np.random.seed(1)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_excel('../datasets/credit-card-defaulters-taiwan/default of credit card clients.xls', header=1)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
    "\n",
    "# Load dataset\n",
    "X = data.drop(['default payment next month', 'ID'], axis=1)\n",
    "y = data['default payment next month']\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "y_train = y_train.values  # Convert pandas series to NumPy array\n",
    "y_test = y_test.values\n",
    "\n",
    "# Create a PyTorch Dataset\n",
    "class CreditDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # Reshape y to be column vector\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = 64\n",
    "train_dataset = CreditDataset(X_train, y_train)\n",
    "test_dataset = CreditDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model in batches\n",
    "num_epochs = 125\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()  # Set model to evaluation mode\n",
    "y_pred_probs = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        y_pred_probs.extend(outputs.numpy())  # Store predictions\n",
    "        y_true.extend(batch_y.numpy())  # Store true labels\n",
    "\n",
    "# Convert predictions to NumPy array\n",
    "y_pred_probs = np.array(y_pred_probs).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = ((y_pred_probs > 0.5) == y_true).mean()\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Compute ROC AUC score\n",
    "roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "# Plot ROC curve\n",
    "RocCurveDisplay.from_predictions(y_true, y_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SHAP to explain Neural network predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Use existing NumPy arrays\n",
    "X_train_np = X_train\n",
    "X_test_np = X_test\n",
    "\n",
    "# Reduce background dataset for SHAP (using 200 samples instead of full training set)\n",
    "background = shap.kmeans(X_train_np, 200)\n",
    "\n",
    "# Wrapper function for PyTorch model inference\n",
    "def model_wrapper(data):\n",
    "    with torch.no_grad():\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "        preds = model(data_tensor).detach().numpy()\n",
    "        #print(\"Model predictions shape:\", preds.shape)  # Should be (n_samples, n_classes)\n",
    "        return preds\n",
    "\n",
    "# Use KernelExplainer with reduced background\n",
    "explainer = shap.KernelExplainer(model_wrapper, background)\n",
    "\n",
    "# Explain only a small subset of the test set (e.g., 50 samples)\n",
    "X_test_sample = X_test_np[:50]\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(\"X_test_sample shape:\", X_test_sample.shape)  # Should be (n_samples, n_features)\n",
    "print(\"SHAP values shape:\", np.array(shap_values).shape)  # Should match (n_samples, n_features)\n",
    "\n",
    "# Remove extra dimension if shap_values is (50, 23, 1)\n",
    "shap_values = np.array(shap_values)  # Convert to NumPy array\n",
    "if shap_values.shape[-1] == 1:\n",
    "     shap_values = shap_values.squeeze(-1)  # Remove last dimension\n",
    "\n",
    "# Ensure correct shape\n",
    "print(\"Updated SHAP values shape:\", shap_values.shape)  # Should be (50, 23)\n",
    "\n",
    "\n",
    "# Generate SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_test_sample, feature_names=data.columns[1:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What Do the Axes Mean?\n",
    "-\tY-axis (Feature Names): Lists the most important features from top to bottom, ranked by their impact on the model’s predictions.\n",
    "-\tX-axis (SHAP Value): Represents the impact of each feature on the model’s prediction.\n",
    "-\tA positive SHAP value increases the probability of default.\n",
    "-\tA negative SHAP value decreases the probability of default.\n",
    "\n",
    "2. What Do the Colors Mean?\n",
    "\t-\tRed: High feature value (e.g., high credit limit, high bill amount, etc.)\n",
    "\t-\tBlue: Low feature value (e.g., low credit limit, low bill amount, etc.)\n",
    "\t-\tPurple: Intermediate values.\n",
    "    - If a red dot is positioned to the right, it means that a higher value of that feature increases the default risk. If it’s on the left, it decreases the risk.\n",
    "\n",
    "3. The top-ranked features contribute the most to the model’s prediction. In this case:\n",
    "\t-\t**PAY_0 (Repayment Status for the Most Recent Month):**\n",
    "\t\t- The most significant predictor of default.\n",
    "\t\t- Higher values (more delayed payments) strongly increase default probability (red dots shift to the right).\n",
    "\t \t-  Lower values (on-time payments) reduce the risk (blue dots shift left).\n",
    "\t-\t**LIMIT_BAL (Credit Limit Amount):**\n",
    "\t\t- Higher credit limits seem to reduce the risk of default (red dots slightly shift left).\n",
    "\t\t-\tLower credit limits are more associated with default risk.\n",
    "\t-\t**BILL_AMT4, BILL_AMT1, BILL_AMT6 (Bill Amounts for Different Months):**\n",
    "\t\t-\tLarge bill amounts have mixed impact, depending on other factors.\n",
    "\t\t-\tHigher bills (red) sometimes increase default risk but not always.\n",
    "\t-\t**PAY_4, PAY_3, PAY_2, PAY_5, PAY_6 (Previous Payment Statuses):**\n",
    "\t\t-\tLate payments from earlier months also contribute to default risk.\n",
    "\t\t-\tPattern: More delays → Higher SHAP values → More risk.\n",
    "\n",
    "4. Lesser Impact Features\n",
    "\t-\t**SEX, AGE, MARRIAGE, EDUCATION:**\n",
    "\t\t-\tThese have much lower SHAP values, meaning they don’t significantly influence predictions.\n",
    "\t\t-\tSome impact exists, but it’s much weaker than repayment history."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
