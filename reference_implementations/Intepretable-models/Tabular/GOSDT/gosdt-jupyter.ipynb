{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Sparse Decision Tree Optimization via Reference Ensembles \n",
    "\n",
    "Also known as GHOUL (Guessing Helps Optimize Upper and Lower Bounds) or GOSDT with Guesses.\n",
    "\n",
    "While GOSDT makes it possible to find optimal decision trees, its computation time on data sets with many features can be large. GHOUL introduces smart guessing strategies that leverage black box models to reduce the computatoin time by multiple orders of magnitude, while providing bounds on how far the resulting trees can deviate from the black box's accuracy and expressiveness.\n",
    "\n",
    "\n",
    "### References:\n",
    "1. [McTavish, H., Zhong, C., Achermann, R., Karimalis, I., Chen, J., Rudin, C., & Seltzer, M. (2022). Fast Sparse Decision Tree Optimization via Reference Ensembles. Proceedings of the AAAI Conference on Artificial Intelligence, 36(9), 9604-9613.](https://doi.org/10.1609/aaai.v36i9.21194)\n",
    "   \n",
    "2. [Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, and Margo Seltzer. 2020. Generalized and scalable optimal sparse decision trees. In Proceedings of the 37th International Conference on Machine Learning (ICML'20), Vol. 119. JMLR.org, Article 571, 6150–6160.](http://proceedings.mlr.press/v119/lin20g/lin20g.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Load packages**\n",
    "\n",
    "GOSDT can be installed using `uv add gosdt` or `pip add gosdt`. \n",
    "\n",
    "### NOTE:\n",
    "The  gosdt package  tries to import `check_X_y from sklearn.base`, but in newer versions of scikit-learn, \n",
    "`check_X_y` is located in `sklearn.utils.validation`. \n",
    "As a fix, you can edit\n",
    "`.venv/lib/python3.10/site-packages/gosdt/_threshold_guessing.py`\n",
    "\n",
    "and change it from:\n",
    "```\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, check_X_y \n",
    "```\n",
    "to \n",
    "\n",
    "```\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y\n",
    "```\n",
    "\n",
    "We prefer this solution so as not the break the dependency on scikit-learn version for other examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gosdt import ThresholdGuessBinarizer, GOSDTClassifier\n",
    "from utils.visualize import visualize_gosdt_tree\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.Read the Compas recidivism dataset**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv(\"datasets/compas.csv\", sep=\",\")\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "h = df.columns[:-1]\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2021)\n",
    "print(\"X train shape:{}, X test shape:{}\".format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Set up hyperparameters for GOSDT**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "GBDT_N_EST = 50\n",
    "GBDT_MAX_DEPTH = 1\n",
    "REGULARIZATION = 0.001\n",
    "SIMILAR_SUPPORT = False\n",
    "DEPTH_BUDGET = 6\n",
    "TIME_LIMIT = 120\n",
    "VERBOSE = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.Guess the thresholds**\n",
    "\n",
    "**ThresholdGuessBinarizer:**\tA technique that converts continuous features into binary splits based on estimated thresholds using Gradient Boosted Decision Trees (GBDTs).\n",
    "\n",
    "Example: (e.g., \"Age > 50\" → 1, \"Age ≤ 50\" → 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Guess Thresholds\n",
    "X_train = pd.DataFrame(X_train, columns=h)\n",
    "X_test = pd.DataFrame(X_test, columns=h)\n",
    "enc = ThresholdGuessBinarizer(n_estimators=GBDT_N_EST, max_depth=GBDT_MAX_DEPTH, random_state=2021)\n",
    "enc.set_output(transform=\"pandas\")\n",
    "X_train_guessed = enc.fit_transform(X_train, y_train)\n",
    "X_test_guessed = enc.transform(X_test)\n",
    "print(f\"After guessing, X train shape:{X_train_guessed.shape}, X test shape:{X_test_guessed.shape}\")\n",
    "print(f\"train set column names == test set column names: {list(X_train_guessed.columns) == list(X_test_guessed.columns)}\")\n",
    "\n",
    "# Store feature names for later use\n",
    "feature_names = X_train_guessed.columns.tolist()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Guess the lower bounds**\n",
    "\n",
    "-\tA Gradient Boosting Classifier is trained on the threshold-binarized dataset (X_train_guessed).\n",
    "-\tPredicts labels (warm_labels) for training data, which are used as a starting point for GOSDT optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Guess Lower Bounds\n",
    "enc = GradientBoostingClassifier(n_estimators=GBDT_N_EST, max_depth=GBDT_MAX_DEPTH, random_state=42)\n",
    "enc.fit(X_train_guessed, y_train)\n",
    "warm_labels = enc.predict(X_train_guessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Train GOSDT classifier**\n",
    "\n",
    "Creates a GOSDT classifier with:\n",
    "-\tRegularization (REGULARIZATION) → Controls tree complexity (penalizes overfitting).\n",
    "-\tDepth budget (DEPTH_BUDGET) → Limits tree depth to control model size.\n",
    "-\tTime limit (TIME_LIMIT) → Stops training after a certain period.\n",
    "-\tWarm labels (y_ref=warm_labels) → Uses GBDT predictions to speed up convergence.\n",
    "-\tFits the GOSDT model on the threshold-binarized training data (X_train_guessed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train the GOSDT classifier\n",
    "clf = GOSDTClassifier(regularization=REGULARIZATION, similar_support=SIMILAR_SUPPORT, time_limit=TIME_LIMIT, depth_budget=DEPTH_BUDGET, verbose=VERBOSE) \n",
    "clf.fit(X_train_guessed, y_train, y_ref=warm_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Evaluate the model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate the model\n",
    "print(\"Evaluating the model, extracting tree and scores\", flush=True)\n",
    "\n",
    "print(f\"Model training time: {clf.result_.time}\")\n",
    "print(f\"Training accuracy: {clf.score(X_train_guessed, y_train)}\")\n",
    "print(f\"Test accuracy: {clf.score(X_test_guessed, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Visualize the GOSDT model**\n",
    "\n",
    "We are using the `graphviz` library to visualize the tree. Ensure graphviz executable is installed and in the path\n",
    "```\n",
    "# using macos, install with brew install graphviz\n",
    "# using ubuntu, install with sudo apt-get install graphviz\n",
    "# using windows, download and install from https://graphviz.org/download/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract feature names from dataset\n",
    "feature_names = X_train_guessed.columns.tolist()  # Assuming X_train is your DataFrame\n",
    "print(feature_names)\n",
    "\n",
    "# Parse the tree JSON string into a dictionary\n",
    "tree_dict = json.loads(clf.result_.model)\n",
    "\n",
    "# Visualize the tree\n",
    "dot = visualize_gosdt_tree(tree_dict, feature_names)\n",
    "dot.render(\"gosdt_tree\", format=\"png\", view=False)  # Save and open\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load the generated tree diagram**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the tree by loading the png file saved in ipython notebook\n",
    "from IPython.display import Image\n",
    "Image(\"gosdt_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. How to Read This Tree?**\n",
    "\n",
    "- Each internal node (oval) represents a **decision rule** (e.g., `\"age ≤ 21.5?\"`).\n",
    "- Each leaf node (rectangle) represents a **prediction**:\n",
    "  - `\"Class: 1\"` → **High risk of recidivism** (re-offense likely).\n",
    "  - `\"Class: 0\"` → **Low risk of recidivism** (re-offense unlikely).\n",
    "- Edges labeled “Yes” or “No” show **decision paths**.\n",
    "- Loss values indicate the **error at that node**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Decision Process\n",
    "\n",
    "### **1.Root Node: `age ≤ 21.5?`**\n",
    "- **If “Yes”** → The person is **young**, **high risk of recidivism** (**Class: 1, Loss: 0.0255**).\n",
    "- **If “No”** → Check **`priors_count`** (past convictions).\n",
    "\n",
    "### **2️.`priors_count ≤ 1.5?`**\n",
    "- **If “Yes”** → The person has **few prior offenses**, **low risk** (**Class: 0, Loss: 0.1229**).\n",
    "- **If “No”** → Check **juvenile criminal history**.\n",
    "\n",
    "### **3️.`juvenile_crimes ≤ 0.5?`**\n",
    "- **If “No”** → **High risk** (**Class: 1, Loss: 0.0262**).\n",
    "- **If “Yes”** → Check **age**.\n",
    "\n",
    "### **4️.`age ≤ 38.5?`**\n",
    "- **If “No”** → Check **`priors_count ≤ 7.5?`**:\n",
    "  - **If “No”** → **High risk** (**Class: 1, Loss: 0.0188**).\n",
    "  - **If “Yes”** → **Low risk** (**Class: 0, Loss: 0.0355**).\n",
    "- **If “Yes”** → Check **`age ≤ 27.5?`**:\n",
    "  - **If “Yes”** → **High risk** (**Class: 1, Loss: 0.0288**).\n",
    "  - **If “No”** → Check **`priors_count ≤ 3.5?`**:\n",
    "    - **If “No”** → **High risk** (**Class: 1, Loss: 0.0349**).\n",
    "    - **If “Yes”** → **Low risk** (**Class: 0, Loss: 0.0252**).\n",
    "\n",
    "**Key observations:**\n",
    "\n",
    "1. Age is the strongest predictor:\n",
    "\t-\tYounger individuals (≤21.5) are more likely to re-offend.\n",
    "\t-\tOlder individuals with few priors are less likely to re-offend.\n",
    "2. Prior offenses (priors_count) are critical:\n",
    "\t-\tIf priors_count ≤ 1.5, recidivism is unlikely.\n",
    "\t-\tIf priors_count ≥ 7.5, recidivism is very likely.\n",
    "3.\tJuvenile crimes (juvenile_crimes) increase risk:\n",
    "\t-\tIf an individual has a juvenile criminal record, their risk of re-offending is higher.\n",
    "4.\tAging out of crime?\n",
    "\t-\tIf age ≥ 38.5 and few priors, the risk decreases significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Contrast with the CART decision tree algorithm**\n",
    "\n",
    "CART (Classification and Regression Trees) is a widely used decision tree algorithm that constructs predictive models for classification and regression tasks. It was introduced by Breiman et al. (1984) and forms the foundation of many modern tree-based models.\n",
    "\n",
    "While tree based decision tree are fundamentally interpretable, this becomes very challenging when the tree becomes too large. We can see this phenomemenon clearly in the following example were we train a conventional decision tree on the same dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train a conventional Decision Tree\n",
    "cart_clf = DecisionTreeClassifier(max_depth=6, random_state=42)  # Limit depth for interpretability\n",
    "cart_clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training accuracy: {cart_clf.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {cart_clf.score(X_test, y_test)}\")\n",
    "\n",
    "# Visualize the Conventional Decision Tree\n",
    "plt.figure(figsize=(18, 14))\n",
    "plot_tree(cart_clf, feature_names=X.columns, class_names=[\"0\", \"1\"], filled=True, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GOSDT vs. Conventional Decision Trees (CART)\n",
    "\n",
    "| **Aspect**            | **GOSDT (Generalized Optimal Sparse Decision Trees)** | **CART (Conventional Decision Tree)** |\n",
    "|----------------------|--------------------------------------------------|-------------------------------------|\n",
    "| **Optimization Approach** | **Global Optimization** (Considers all possible trees for best regularized fit) | **Greedy Optimization** (Splits at best feature locally, no guarantee of global optimality) |\n",
    "| **Interpretability**  | **Sparse & Optimal Tree** (Minimizes complexity while preserving accuracy) | **Larger Tree, More Splits** (Prone to overfitting unless pruned) |\n",
    "| **Regularization**    | **Uses explicit regularization** (`regularization=0.001`) to encourage sparsity | **Uses pruning (`max_depth`)** to prevent overfitting |\n",
    "| **Training Time**     | **Longer** (Solves an optimization problem) | **Faster** (Uses greedy heuristics) |\n",
    "| **Accuracy**         | **More stable, lower variance** (Regularization prevents overfitting) | **Higher variance, more prone to overfitting** |\n",
    "| **Feature Selection** | **Automatic feature selection** due to global optimization | **Uses all features unless pruned** |\n",
    "| **Depth of Tree**     | **Optimally limited** based on dataset complexity | **Can be very deep unless restricted** |\n",
    "| **Use Case**         | When **interpretability & optimality** are critical | When **speed & good enough results** are needed |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
