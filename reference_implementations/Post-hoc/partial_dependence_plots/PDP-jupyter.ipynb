{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Model Predictions with Partial Dependence Plots (PDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction**\n",
    "- In this notebook, we will explore Partial Dependence Plots (PDP), a powerful post hoc explainability method.\n",
    "PDP helps us understand how a feature or a pair of features impacts the predictions of a machine learning model.\n",
    "\n",
    "- Partial dependence plots (PDP) show the **dependence between the target response and a set of input features of interest**, marginalizing over the values of all other input features (the ‘complement’ features). Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest.\n",
    "\n",
    "- Partial dependence plots are global explainability method which considers all instances and shows the global relationship of a feature with the predicted outcome. \n",
    "\t1.\tModel Prediction:\n",
    "\t\t-\tA machine learning model predicts an output  $f(x)$ , where  x  is a vector of features  $x = [x_1, x_2, \\dots, x_d]$ .\n",
    "\t2.\tFixing the Feature of Interest:\n",
    "\t\t- To compute the partial dependence for one feature (e.g.,  $x_s$ ), we fix its value to  $x_s^*$  while allowing all other features ( $x_{-s}$ ) to vary across their observed values in the dataset.\n",
    "\t3.\tAveraging Over All Other Features:\n",
    "\t\t- For each fixed value of the feature of interest  $x_s^*$ , the model predictions are averaged over the observed combinations of the other features  $x_{-s}$ . This removes the influence of the specific values of  $x_{-s}$ , capturing only the marginal effect of  $x_s$.\n",
    "\t\t\n",
    "- Mathematically, for a single feature  $x_s$ , the partial dependence is:\n",
    "\n",
    "\t$\\hat{f}{\\text{PDP}}(x_s^*) = \\frac{1}{n} \\sum_{i=1}^{n} f(x_s^*, x_{-s}^{(i)})$\n",
    "\n",
    "where:\n",
    "- $x_s^*$ : Fixed value of the feature of interest.\n",
    "- $x_{-s}^{(i)}$ : Observed values of all other features for the i-th data point.\n",
    "- $n$ : Total number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Load and Explore the Dataset**\n",
    "Here we first load the California Housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Display dataset summary\n",
    "print(\"Dataset Overview:\")\n",
    "display(X.head())\n",
    "print(\"\\nTarget (y) Overview:\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Data Preprocessing**\n",
    "\n",
    "Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Train a Linear Regression Model**\n",
    "\n",
    "The **interpretability** of linear regression is one of its greatest strengths. The model’s equation offers clear coefficients that illustrate the influence of each independent variable on the dependent variable, enhancing our understanding of the underlying relationships. Its simplicity is a significant advantage; linear regression is transparent, easy to implement, and serves as a foundational concept for more advanced algorithms. Let's initialize and train the model using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Evaluation:\\nMean Squared Error: {mse:.2f}\\nR-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Compute Partial Dependence Plots (PDP)**\n",
    "\n",
    "- Due to the limits of human perception, the size of the set of input features of interest must be small (usually, one or two) thus the input features of interest are usually chosen among the most important features.\n",
    "- Here we select a feature for one-way PDP visualization. In this case, 'MedInc' - Median Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
    "\n",
    "# Features to plot (indices of features)\n",
    "features_to_plot = [0, 1, 4, 6]  # Example: 'MedInc', 'HouseAge', 'AveRooms', 'AveOccup'\n",
    "feature_names_to_plot = [data.feature_names[i] for i in features_to_plot]\n",
    "\n",
    "# Initialize subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 5))  # 2x2 grid of subplots\n",
    "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "# Loop through each feature\n",
    "for i, feature_index in enumerate(features_to_plot):\n",
    "    # Compute partial dependence\n",
    "    pdp_results = partial_dependence(\n",
    "        model,\n",
    "        X_train_scaled,\n",
    "        [feature_index],\n",
    "        grid_resolution=50  # Increase resolution for smoother plots\n",
    "    )\n",
    "    \n",
    "    # Get the scaled feature grid and averaged predictions\n",
    "    scaled_feature_values = pdp_results['grid_values'][0]  # Feature grid (scaled values)\n",
    "    averaged_predictions = pdp_results['average'][0]  # Partial dependence values\n",
    "\n",
    "    # Reverse the scaling for the feature grid\n",
    "    feature_mean = scaler.mean_[feature_index]  # Mean of the feature\n",
    "    feature_std = np.sqrt(scaler.var_[feature_index])  # Standard deviation of the feature\n",
    "    unscaled_feature_values = scaled_feature_values * feature_std + feature_mean  # Reverse scaling\n",
    "\n",
    "    # Plot PDP on the corresponding subplot\n",
    "    axes[i].plot(unscaled_feature_values, averaged_predictions, label='Partial Dependence')\n",
    "    axes[i].set_xlabel(f\"{feature_names_to_plot[i]} (Original Scale)\")\n",
    "    axes[i].set_ylabel(\"Partial Dependence\")\n",
    "    axes[i].set_title(f\"PDP for {feature_names_to_plot[i]}\")\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Two-feature Partial Dependence Plot**\n",
    "\n",
    "Instead of looking at each feature separately, we can also visualize a two-feature PDP. Here we select two features for PDP (e.g., 'MedInc' and 'AveRooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_plot_2d = [(0, 4)]  # Indices of 'MedInc' and 'Population'\n",
    "\n",
    "# Compute and plot 2D PDP\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    model, X_train_scaled, features_to_plot_2d, feature_names=data.feature_names\n",
    ")\n",
    "plt.title(\"2D Partial Dependence Plot: Median Income vs Population\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8.Interpretation**\n",
    "\n",
    "- Higher Median Income (MedInc): As MedInc increases (moves rightward on the X-axis), the predicted values increase (lighter colors appear), reflecting a positive relationship between median income and the target variable (e.g., house price).\n",
    "- Population Effect (Population): The Y-axis effect of Population appears weaker since the variation in predicted values along this axis (up-down direction) is relatively smaller compared to MedInc.\n",
    "- This plot suggests that **MedInc has a stronger influence on predictions than Population**, as the gradient of the color change is primarily along the X-axis.\n",
    "- The effect of Population appears to be relatively uniform across different levels of MedInc, indicating limited interaction between the two features.\n",
    "- Dominant Feature: MedInc (Median Income) is the dominant factor influencing the model predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. GradientBoostingRegressor\n",
    "\n",
    "Simple Linear Regression is too simple a model for many problems. Let's try a more comples ensemble classifier on the same dataset. We now use the GradientBoostingRegressor model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(learning_rate=0.1,n_estimators=100) \n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Evaluation:\\nMean Squared Error: {mse:.2f}\\nR-squared: {r2:.2f}\")\n",
    "\n",
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
    "\n",
    "# Features to plot (indices of features)\n",
    "features_to_plot = [0, 1, 4, 6]  # Example: 'MedInc', 'HouseAge', 'AveRooms', 'AveOccup'\n",
    "feature_names_to_plot = [data.feature_names[i] for i in features_to_plot]\n",
    "\n",
    "# Initialize subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 5))  # 2x2 grid of subplots\n",
    "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "# Loop through each feature\n",
    "for i, feature_index in enumerate(features_to_plot):\n",
    "    # Compute partial dependence\n",
    "    pdp_results = partial_dependence(\n",
    "        model,\n",
    "        X_train_scaled,\n",
    "        [feature_index],\n",
    "        grid_resolution=50  # Increase resolution for smoother plots\n",
    "    )\n",
    "    \n",
    "    # Get the scaled feature grid and averaged predictions\n",
    "    scaled_feature_values = pdp_results['grid_values'][0]  # Feature grid (scaled values)\n",
    "    averaged_predictions = pdp_results['average'][0]  # Partial dependence values\n",
    "\n",
    "    # Reverse the scaling for the feature grid\n",
    "    feature_mean = scaler.mean_[feature_index]  # Mean of the feature\n",
    "    feature_std = np.sqrt(scaler.var_[feature_index])  # Standard deviation of the feature\n",
    "    unscaled_feature_values = scaled_feature_values * feature_std + feature_mean  # Reverse scaling\n",
    "\n",
    "    # Plot PDP on the corresponding subplot\n",
    "    axes[i].plot(unscaled_feature_values, averaged_predictions, label='Partial Dependence')\n",
    "    axes[i].set_xlabel(f\"{feature_names_to_plot[i]} (Original Scale)\")\n",
    "    axes[i].set_ylabel(\"Partial Dependence\")\n",
    "    axes[i].set_title(f\"PDP for {feature_names_to_plot[i]}\")\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretations of the PDP for GradientBoostingRegressor model:**\n",
    "\n",
    "- PDP Medinc:\n",
    "    - As MedInc increases, the model’s prediction also increases consistently.\n",
    "    - Median Income is reflected as a significant positive predictor\n",
    "    - nearlty linear trend indicates model learns a direct relationship between income and target without strong non-linear interactions\n",
    "\n",
    "- PDP HouseAge\n",
    "    - step-wise increasing pattern\n",
    "    - For house ages up to around 30 years, the effect is nearly flat. Beyond this, the partial dependence increases gradually and then sharply for houses over 50 years old.\n",
    "    - Older houses (50+ years) contribute positively to the target, potentially because older properties might be in more desirable or established areas.\n",
    "\n",
    "- PDP Population\n",
    "    - Population has a relatively weak effect on the target variable for smaller values (<1500).\n",
    "    - For larger populations, the partial dependence stabilizes, suggesting that the model considers blocks with higher populations more favorable, but only up to a certain point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Conclusion**\n",
    "\n",
    "- Partial Dependency Plots are one of the baselines for post-hoc explainability. \n",
    "- The main weakness of PDP:\n",
    "    - the assumption that each feature is independant may be violated in many problems as features may be correlated\n",
    "    - PDP shows average effects of each feature on the target variable - so it might miss nuances or localized patterns - oversimplyfying main effects\n",
    "    - cannot capture interactions between features well.\n",
    "    - PDPD works best for continuous feature, less effective on categorical features\n",
    "    - PDP is a global explainability tool, so it cannot be used for instance-level local explanations\n",
    "- Are there better alternatives ?\n",
    "    - Sure there are. One such technique is Accumulated Average Effects Plot (ALE) which we shall see next.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
