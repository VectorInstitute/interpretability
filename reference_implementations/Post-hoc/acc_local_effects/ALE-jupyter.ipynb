{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Model Predictions with Accumulated Local Effects Plot (ALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "- In this notebook, we shall explore ALE, a post-hoc explainability method that examines the relationship between a feature (or features) and the model’s predictions, while accounting for potential dependencies between features -- which PDP as we saw may not be able to do. \n",
    "- What are the key concepts of ALE ?\n",
    "\n",
    "    - ALE focuses on local effects i.e. examining the small,  localized changes of a feature while keeping all other features constant (or marginalizing over them)\n",
    "    - The local effects are integrated (accumulated) over the feature’s value range to understand the global effect of the feature on predictions.\n",
    "    - ALE plots are centered around zero ensuring the visualized effects represent deviations from the average prediction.\n",
    "\n",
    "- At a high level, these are the steps to plot the ALE:\n",
    "\n",
    "    1.\t**Bin the Feature:**\n",
    "\t    -\tDivide the range of the feature into intervals (bins).\n",
    "\t2.\t**Compute Local Effects:**\n",
    "\t    -\tFor each interval, calculate the effect of moving the feature value from one side of the interval to the other while keeping other features constant.\n",
    "\t3.\t**Accumulate Effects:**\n",
    "\t    - \tIntegrate (accumulate) the local effects across all intervals.\n",
    "\t4.\t**Center the Plot:**\n",
    "\t    -\tSubtract the mean of the accumulated effects to center the ALE curve around zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Import the necessary libraries**\n",
    "\n",
    "We are specifically ging to be using PyALE, a library in Python that implements ALE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score,auc,roc_curve,roc_auc_score  \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from PyALE import ale\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Loading the dataset**\n",
    "\n",
    "We are using a Bank Marketing Dataset from a Portuguese banking institution. There are 16 direct marketing (phone calls related) features from 45,211 clients and task is to  classify if client will subscribe a term deposit or not. \n",
    "Dataset URL:  https://archive.ics.uci.edu/static/public/222/bank+marketing.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/bank-full.csv', delimiter=';')   \n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.Pre-process the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Automatically determine categorical and numerical features\n",
    "categorical_features = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numerical_features = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "print(\"Categorical Features:\", categorical_features)\n",
    "print(\"Numerical Features:\", numerical_features)\n",
    "\n",
    "\n",
    "# Remove the target variable (if included in the dataset)\n",
    "target_variable = \"y\"\n",
    "if target_variable in categorical_features:\n",
    "    categorical_features.remove(target_variable)\n",
    "if target_variable in numerical_features:\n",
    "    numerical_features.remove(target_variable)\n",
    "    \n",
    "# Define the ColumnTransformer\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", scaler, numerical_features),   # Scale numerical features\n",
    "        (\"cat\", one_hot_encoder, categorical_features)  # One-hot encode categorical features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Convert true labels to binary format\n",
    "y= (y == 'yes').astype(int)\n",
    "\n",
    "# Apply the transformation to features\n",
    "processed_X = preprocessor.fit_transform(X)\n",
    "\n",
    "# Extract the fitted OneHotEncoder\n",
    "one_hot_encoder = preprocessor.named_transformers_[\"cat\"]\n",
    "\n",
    "# Get one-hot encoded column names\n",
    "one_hot_encoded_columns = one_hot_encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine with scaled numerical column names\n",
    "final_column_names = numerical_features + list(one_hot_encoded_columns)\n",
    "print(final_column_names)\n",
    "\n",
    "# Create a DataFrame with the transformed data\n",
    "processed_X_df = pd.DataFrame(processed_X, columns=final_column_names)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(processed_X_df.head())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_X_df, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Let's train a Logistic Regression Model on this dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "logit_fun = model.decision_function\n",
    "proba_fun = model.predict_proba\n",
    "\n",
    "\n",
    "y_probs = proba_fun(X_test)[:, 1]\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = (y_probs >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc_score = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy)\n",
    "print(\"AUC Score:\", auc_score)\n",
    "\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.ALE plots**\n",
    "\n",
    "Let us now use pyALE to explain the predictions of this classifier. We will start by analysisng the effects of individial features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "ale_effects_age = ale(X_train, model=model, feature=[\"age\"], grid_size=50,include_CI=False,fig=fig,ax=ax1)\n",
    "ale_effects_loan = ale(X_train, model=model, feature=[\"balance\"], grid_size=50,include_CI=False,fig=fig,ax=ax2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ALE:  age feature\n",
    "\t- \tThe plot shows a slight downward trend as age increases.\n",
    "\t- \tThis means that higher age values reduce the model’s prediction marginally.\n",
    "\t- \tThe rug plot at the bottom indicates where most of the data points lie (densely clustered near 0). Sparse data at higher age values suggests limited information for the model to learn strong effects in this range.\n",
    "2. ALE: balance feature\n",
    "\t- \tAs balance increases, the ALE curve shows a sharp positive trend.\n",
    "\t- \tThis suggests that higher values of balance significantly increase the model’s prediction.\n",
    "\t- \tRug plot (bottom) shows most data points are concentrated at lower values of balance, with sparse data at higher values. The steep rise at high balance values should be interpreted cautiously, as there might be fewer data points in this region.\n",
    "\n",
    "3. Interpretation\n",
    "\t- \tage: Further domain knowledge could explain why older individuals contribute negatively to the outcome.\n",
    "\t- \tbalance: This is likely a highly important feature for the model. Ensure that the model is not overfitting to the few high-balance examples, as the sharp rise suggests potential sensitivity in this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "ale_effects_age = ale(X_train, model=model, feature=[\"loan_yes\"], grid_size=50,include_CI=False,fig=fig,ax=ax1)\n",
    "ale_effects_loan = ale(X_train, model=model, feature=[\"marital_married\"], grid_size=50,include_CI=False,fig=fig,ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These ALE plots for categorical variables show how the presence or absence of specific categories (encoded as 0 and 1) influence the model’s predictions. Here’s how to interpret them:\n",
    "\n",
    "1. Left Plot: loan_yes\n",
    "\t- \tEffect on Prediction:\n",
    "\t\t- \tWhen loan_yes = 0 (no loan), the effect on the model’s prediction is positive (~0.0025).\n",
    "\t\t- \tWhen loan_yes = 1 (has a loan), the effect is significantly negative (~-0.015).\n",
    "\t- \tInterpretation:\n",
    "\t\t- \tThe model predicts lower outcomes for individuals who have a loan (loan_yes = 1) compared to those who do not.\n",
    "\t\t- \tThis could indicate that having a loan is a negative factor for the prediction target (e.g., likelihood of success, repayment, or a similar outcome).\n",
    "\t- \tDistribution:\n",
    "\t\t- \tThe light blue bar in the background indicates the number of samples (size) for each category.\n",
    "\t\t- \tMost samples are concentrated in the loan_yes = 0 category, with fewer samples in loan_yes = 1.\n",
    "\n",
    "2. Right Plot: marital_married\n",
    "\t- \tEffect on Prediction:\n",
    "\t\t- \tWhen marital_married = 0 (not married), the effect on the model’s prediction is positive (~0.01).\n",
    "\t\t- \tWhen marital_married = 1 (married), the effect is negative (~-0.007).\n",
    "\t- \tInterpretation:\n",
    "\t\t- \tThe model predicts higher outcomes for individuals who are not married compared to those who are.\n",
    "\t\t- \tThis suggests that marital status has a noticeable impact on the target variable, with being married contributing negatively.\n",
    "\t- \tDistribution:\n",
    "\t\t- \tThe light blue bar shows that more samples belong to the marital_married = 1 category, meaning the majority of individuals in the dataset are married.\n",
    "\n",
    "**General Observations**\n",
    "\n",
    "1.\tImbalance in Data Distribution:\n",
    "\t- \tBoth plots show imbalances in the sample size for the two categories.\n",
    "\t- \tThe imbalances could affect the reliability of the ALE effects, especially for categories with fewer samples (loan_yes = 1).\n",
    "2.\tCentered Effects:\n",
    "\t- \tThe effects are centered around 0, making it easier to interpret the relative importance of each category.\n",
    "3.\tImplications:\n",
    "\t- \tFor loan_yes: Having a loan has a strong negative effect, indicating it could be a key feature in the model.\n",
    "\t- \tFor marital_married: Marital status also plays a role, but the negative effect for married individuals suggests an interesting relationship worth investigating further.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
