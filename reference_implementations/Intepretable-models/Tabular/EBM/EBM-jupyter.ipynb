{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable Boosting Machine (EBM)\n",
    "\n",
    "EBMs belong to the family of Generative Additive Models (GAM).EBM is a glassbox model, designed to have accuracy comparable to state-of-the-art machine learning methods like Random Forest and Boosted Trees, while being highly intelligibile and explainable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import libraries and Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret import show\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('../../Post-hoc/datasets/diabetes_130/diabetic_data.csv')\n",
    "X = data.drop(columns=['readmitted'])\n",
    "#we will chnage this to a binarry classification problem. \n",
    "#We will consider readmitted as a positive class if the patient was readmitted within 30 days or after 30 days\n",
    "y = data['readmitted'].apply(lambda x: 'YES' if x in ['<30', '>30'] else 'NO')\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "#missing values will be changed to extreme values like -1000\n",
    "# we will not impute the missing values because that will affect the shape functions computed by EBM\n",
    "X = X.fillna(-1000)\n",
    "\n",
    "# # we will convert the categorical variables to one-hot encoding\n",
    "# X = pd.get_dummies(X)\n",
    "\n",
    "# # we will drop the columns that have only one unique value\n",
    "# X = X.drop(columns=['examide_No', 'citoglipton_No'])\n",
    "\n",
    "# we will drop patient_nbr because it is a unique identifier and encounter_id because it is a random number\n",
    "X = X.drop(columns=['patient_nbr', 'encounter_id'])\n",
    "\n",
    "# use label encoder to change YES to 1 and NO to 0\n",
    "y = y.apply(lambda x: 1 if x == 'YES' else 0)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "display(df.head())\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Train and validate the EBM model.**\n",
    "\n",
    "We will use the `Interpret.ml` toolkit which includes the official implementation of EBMs. Uisng the toolkit is very simple and straightforward as you will see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "\n",
    "def train_or_load_ebm(X, y, model_path='ebm_model.pkl', pretrained=False):\n",
    "    if pretrained and os.path.exists(model_path):\n",
    "        ebm = joblib.load(model_path)\n",
    "        print(\"Loaded pretrained model.\")\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "        # EBM hyperparameters\n",
    "        params = {\n",
    "            'max_leaves': 3,\n",
    "            'learning_rate': 0.03,\n",
    "            'interactions': 0.5, # 50% of features can interact\n",
    "            'random_state': 42,\n",
    "            'smoothing_rounds': 100,\n",
    "\n",
    "        }\n",
    "\n",
    "        # Train EBM with feature names\n",
    "        ebm = ExplainableBoostingClassifier(**params, feature_names=X.columns)\n",
    "        ebm.fit(X_train, y_train)\n",
    "\n",
    "        # Save the model\n",
    "        joblib.dump(ebm, model_path)\n",
    "        print(\"Trained and saved new model.\")\n",
    "\n",
    "        # Predict probabilities and classes for the test set\n",
    "        y_pred_proba = ebm.predict_proba(X_test)[:, 1]\n",
    "        y_pred = ebm.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        sensitivity = recall_score(y_test, y_pred)\n",
    "        specificity = recall_score(y_test, y_pred, pos_label=0)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        # Plot ROC Curve\n",
    "        axes[0].plot(fpr, tpr, color='blue', label=f'AUC-ROC = {auc_roc:.2f}')\n",
    "        axes[0].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "        axes[0].set_xlabel('False Positive Rate')\n",
    "        axes[0].set_ylabel('True Positive Rate')\n",
    "        axes[0].set_title('ROC Curve')\n",
    "        axes[0].legend()\n",
    "\n",
    "        # Plot Confusion Matrix\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "        axes[1].set_xlabel('Predicted')\n",
    "        axes[1].set_ylabel('Actual')\n",
    "        axes[1].set_title('Confusion Matrix')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f'Accuracy: {accuracy:.2f}')\n",
    "        print(f'Sensitivity (Recall): {sensitivity:.2f}')\n",
    "        print(f'Specificity: {specificity:.2f}')\n",
    "\n",
    "    return ebm\n",
    "\n",
    "# Example usage:\n",
    "ebm = train_or_load_ebm(X, y, pretrained=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are reasonable but perhaps can be improved by virtue of hperparameter optimization. The toolkit provides tools for interactive visualization of shape functions for each feature and heatmaps (for 2nd order interactions between features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Explain the model**\n",
    "\n",
    "We will first use the built-in `Interpret.ml` toolkit funcionality for visualizing the global model explanation feature-wise explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_explanation = ebm.explain_global()\n",
    "show(global_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "# # 1. Load the breast cancer dataset and get feature names\n",
    "# data = load_breast_cancer()\n",
    "# X, y = data.data, data.target\n",
    "# feature_names = data.feature_names  # actual names\n",
    "\n",
    "# # 2. Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# # 3. Train an Explainable Boosting Classifier using the actual feature names.\n",
    "# ebm = ExplainableBoostingClassifier(feature_names=feature_names, random_state=42)\n",
    "# ebm.fit(X_train, y_train)\n",
    "\n",
    "# 4. Get term importances and select the top six univariate (main effect) terms.\n",
    "importances = ebm.term_importances(importance_type=\"avg_weight\")\n",
    "term_names = np.array(ebm.term_names_)\n",
    "term_features = ebm.term_features_\n",
    "\n",
    "# Filter for main effects (terms with one feature only)\n",
    "main_effect_indices = [i for i, tf in enumerate(term_features) if len(tf) == 1]\n",
    "main_importances = [(i, importances[i]) for i in main_effect_indices]\n",
    "main_importances_sorted = sorted(main_importances, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Select the top six term indices and their names.\n",
    "top6 = main_importances_sorted[:6]\n",
    "top6_indices = [t[0] for t in top6]\n",
    "top6_names = [term_names[i] for i in top6_indices]\n",
    "\n",
    "# 5. Create a 3x2 grid of subplots.\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# For each of the six selected terms, plot the shape function and feature histogram.\n",
    "for idx, term_idx in enumerate(top6_indices):\n",
    "    feature_idx = ebm.term_features_[term_idx][0]\n",
    "    bins = ebm.bins_[feature_idx][0]\n",
    "    shape_vals = ebm.term_scores_[term_idx]\n",
    "    std_vals = ebm.standard_deviations_[term_idx]  # confidence intervals, if available\n",
    "\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    if isinstance(bins, np.ndarray):\n",
    "        bounds = ebm.feature_bounds_[feature_idx]  # [min, max] for the feature\n",
    "        bin_edges = np.concatenate(([bounds[0]], bins, [bounds[1]]))\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        \n",
    "        # Remove the first bin (\"0\" bin)\n",
    "        bin_centers = bin_centers[1:]\n",
    "        shape_vals = shape_vals[1:]\n",
    "        if std_vals is not None:\n",
    "            std_vals = std_vals[1:]\n",
    "        # Ensure lengths match\n",
    "        min_length = min(len(bin_centers), len(shape_vals), len(std_vals) if std_vals is not None else len(shape_vals))\n",
    "        bin_centers = bin_centers[:min_length]\n",
    "        shape_vals = shape_vals[:min_length]\n",
    "        if std_vals is not None:\n",
    "            std_vals = std_vals[:min_length]\n",
    "        # For histogram, use the remaining bin edges (drop the first and last edge)\n",
    "        new_bin_edges = bin_edges[1:-1]\n",
    "    else:\n",
    "        bin_centers = list(bins.keys())\n",
    "        if bin_centers and bin_centers[0] == 0:\n",
    "            bin_centers = bin_centers[1:]\n",
    "            shape_vals = shape_vals[1:]\n",
    "            if std_vals is not None:\n",
    "                std_vals = std_vals[1:]\n",
    "        new_bin_edges = None\n",
    "\n",
    "    # Plot the shape function.\n",
    "    ax.plot(bin_centers, shape_vals, marker='o', linestyle='-', color='blue',\n",
    "            label='Shape function')\n",
    "    # Instead of error bars, fill the area between (shape_vals - std_vals) and (shape_vals + std_vals)\n",
    "    if std_vals is not None:\n",
    "        lower = shape_vals - std_vals\n",
    "        upper = shape_vals + std_vals\n",
    "        ax.fill_between(bin_centers, lower, upper, color='blue', alpha=0.2)\n",
    "    \n",
    "    ax.set_xlabel(feature_names[feature_idx])\n",
    "    ax.set_ylabel(\"Shape value\", color='blue')\n",
    "    ax.tick_params(axis='y', labelcolor='blue')\n",
    "    ax.set_title(top6_names[idx])\n",
    "    \n",
    "    # Create a twin axis for the feature histogram.\n",
    "    ax2 = ax.twinx()\n",
    "    feat_data = X_train[:, feature_idx]\n",
    "    if new_bin_edges is not None:\n",
    "        ax2.hist(feat_data, bins=new_bin_edges, color='gray', alpha=0.3,\n",
    "                 density=True, label='Feature density')\n",
    "    else:\n",
    "        values, counts = np.unique(feat_data, return_counts=True)\n",
    "        ax2.bar(values, counts / np.sum(counts), color='gray', alpha=0.3,\n",
    "                label='Feature density')\n",
    "    ax2.set_ylabel(\"Density\", color='gray')\n",
    "    ax2.tick_params(axis='y', labelcolor='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
