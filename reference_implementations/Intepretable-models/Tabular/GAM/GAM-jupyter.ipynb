{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Additive Models\n",
    "\n",
    "This Jupyter notebook demonstrates the use of a traditional GAM method using the PyGAM library for a tabular classification and a regression problem.\n",
    "\n",
    "### Classification dataset\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/2/adult\n",
    "\n",
    "\n",
    "### Regression Dataset \n",
    "\n",
    "https://www.kaggle.com/datasets/mirichoi0218/insurance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **1.Import modules and dataset**\n",
    "\n",
    "\n",
    "The dataset we are using is the 1994 Census database with minimal missing values. The task is a binary classification i.e. predicting if an individual's annual income is over $50,000.\n",
    "\n",
    "| Column          | Description                                                                 |\n",
    "|-----------------|-----------------------------------------------------------------------------|\n",
    "| age             | Age of the individual                                                       |\n",
    "| workclass       | Type of employment (e.g., Private, Self-emp-not-inc, etc.)                  |\n",
    "| fnlwgt          | Final weight, a measure of the number of people the observation represents  |\n",
    "| education       | Highest level of education achieved (e.g., Bachelors, HS-grad, etc.)        |\n",
    "| education-num   | Number of years of education                                                |\n",
    "| marital-status  | Marital status (e.g., Never-married, Married-civ-spouse, etc.)              |\n",
    "| occupation      | Type of occupation (e.g., Adm-clerical, Exec-managerial, etc.)              |\n",
    "| relationship    | Relationship status (e.g., Not-in-family, Husband, etc.)                    |\n",
    "| race            | Race of the individual (e.g., White, Black, Asian-Pac-Islander, etc.)       |\n",
    "| sex             | Gender of the individual (Male or Female)                                   |\n",
    "| capital-gain    | Capital gains                                                               |\n",
    "| capital-loss    | Capital losses                                                              |\n",
    "| hours-per-week  | Number of hours worked per week                                             |\n",
    "| native-country  | Country of origin (e.g., United-States, Mexico, etc.)                       |\n",
    "| income          | Income level (<=50K or >50K)                                                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score,PrecisionRecallDisplay, RocCurveDisplay, classification_report\n",
    "import os\n",
    "from pygam import LogisticGAM, s, f\n",
    "import joblib\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def load_adult_dataset(directory: str, filename: str = \"adult.data\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the Adult Income dataset from a local directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the local directory containing the dataset file.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The cleaned Adult dataset with column names.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset file not found at {file_path}. Please check the directory path.\")\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        header=None,\n",
    "        names=[\n",
    "            \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "            \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "            \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "        ],\n",
    "        na_values=\" ?\"  # Convert missing values represented as \" ?\" to NaN\n",
    "    )\n",
    "    \n",
    "    # Drop rows with missing values (optional)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load dataset\n",
    "directory = \"../../datasets/adult\"\n",
    "df = load_adult_dataset(directory, filename=\"adult.data\")\n",
    "display(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.Dataset Preprocessing**\n",
    "\n",
    "- Categorical variables are integer-encoded  while the numerical variables are z-score scaled.\n",
    "- Dataset is split into 80% train and 20% validation\n",
    "- There is no need to one-hot encode categorical variables\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Encode target variable\n",
    "    df[\"income\"] = df[\"income\"].apply(lambda x: 1 if x == \" >50K\" else 0)\n",
    "    df.drop(columns=[\"education\"], inplace=True)  # Highly correlated with education-num    \n",
    "    df.drop(columns=[\"fnlwgt\"], inplace=True)  # Not relevant for prediction\n",
    "\n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = [\"workclass\", \"marital-status\", \"occupation\", \n",
    "                            \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "    numerical_features = [\"age\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "    # Convert categorical variables to integers using sklearn's LabelEncoder\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # Save the mapping for future inference\n",
    "    joblib.dump(label_encoders, \"label_encoders.pkl\")\n",
    "\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "    # Save scaling parameters (for future inference)\n",
    "    scaler_mean = scaler.mean_\n",
    "    scaler_var = scaler.var_\n",
    "\n",
    "    return df, categorical_features, numerical_features, scaler_mean, scaler_var\n",
    "\n",
    "# Preprocess the dataset\n",
    "df, categorical_features, numerical_features, scaler_mean, scaler_var = preprocess_data(df)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X = df.drop(\"income\", axis=1)\n",
    "y = df[\"income\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **3.Define the GAM model and start training**\n",
    "\n",
    "- First, we need to define the terms for the Generalized Additive Model (GAM).\n",
    "- We will use **smooth terms** for numerical features and **factor terms** for categorical features.\n",
    "- For classification problems, we must use a **logit link function** and a **binomial distribution**\n",
    "- For simplicity, `pyGAM` has the `LogisticGAM` class that is optimized for classification task incorporating the proper distribution and link functions\n",
    "- The `lam` (lambda) controls the smoothness of the shape functions.\n",
    "\t-\tHigher lambda = More regularization (less flexibility, avoids overfitting)\n",
    "\t-\tLower lambda = More flexibility (higher variance, risk of overfitting)\n",
    "\t-\tWe can use a built-in grid search to find the optimal lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's construct the GAM model\n",
    "\n",
    "# We first initialize the first term\n",
    "terms = s(0)\n",
    "\n",
    "# Add numerical features using smooth functions\n",
    "for i in range(1, len(numerical_features)):\n",
    "    terms += s(i)\n",
    "\n",
    "# Tehn add factor terms for categorical variables dynamically\n",
    "for i in range(len(numerical_features), X_train.shape[1]):\n",
    "    terms += f(i)\n",
    "\n",
    "# Convert to NumPy arrays to avoid index errors\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "\n",
    "# Perform grid search for optimal lambda\n",
    "lam_values = np.logspace(-3, 3, 10)  # Test lambda from 0.001 to 1000\n",
    "gam = LogisticGAM(terms).gridsearch(X_train_np, y_train_np, lam=lam_values)\n",
    "\n",
    "# Model summary\n",
    "print(gam.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.Model Performance on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target variable on the test set\n",
    "y_pred = gam.predict(X_val)\n",
    "y_pred_proba = gam.predict_proba(X_val)\n",
    "\n",
    "# Calculate the performance metrics including F1 score, ROC_AUC and confusion matrix\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Precision-Recall Display\n",
    "PrecisionRecallDisplay.from_predictions(y_val, gam.predict_proba(X_val), ax=axs[0])\n",
    "axs[0].set_title('Precision-Recall Curve - Validation Set')\n",
    "\n",
    "# ROC Curve Display\n",
    "RocCurveDisplay.from_predictions(y_val, gam.predict_proba(X_val), ax=axs[1])\n",
    "axs[1].set_title('ROC Curve - Validation Set')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Classification Report - Validation Set:\\n\", classification_report(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Perforamance on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now preprocess the actual test set and evaluate the model on the test set\n",
    "\n",
    "# Load the test set\n",
    "directory = \"../../datasets/adult\"\n",
    "df_test = load_adult_dataset(directory, filename=\"adult.test\")\n",
    "display(df_test.head())\n",
    "\n",
    "# Convert 'age' column to numeric\n",
    "df_test['age'] = pd.to_numeric(df_test['age'], errors='coerce')\n",
    "\n",
    "\n",
    "#preprocess the test set\n",
    "def preprocess_test_data(df, label_encoders, scaler_mean, scaler_var):\n",
    "    # Encode target variable\n",
    "    df[\"income\"] = df[\"income\"].apply(lambda x: 1 if x == \" >50K.\" else 0)\n",
    "    \n",
    "    df.drop(columns=[\"education\"], inplace=True)  # Highly correlated with education-num\n",
    "    df.drop(columns=[\"fnlwgt\"], inplace=True)  # Not relevant for prediction\n",
    "\n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = [\"workclass\", \"marital-status\", \"occupation\", \n",
    "                            \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "    \n",
    "    numerical_features = [\"age\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "    # Convert categorical variables to integers using the saved LabelEncoders\n",
    "    label_encoders = joblib.load(\"label_encoders.pkl\")\n",
    "    for col in categorical_features:\n",
    "        le = label_encoders[col]\n",
    "        df[col] = le.transform(df[col])\n",
    "\n",
    "    # Scale numerical features using the saved scaling parameters\n",
    "    df[numerical_features] = (df[numerical_features] - scaler_mean) / np.sqrt(scaler_var)\n",
    "    \n",
    "    return df\n",
    "\n",
    "label_encoders = joblib.load(\"label_encoders.pkl\")\n",
    "# Preprocess the test set\n",
    "df_test = preprocess_test_data(df_test, label_encoders, scaler_mean, scaler_var)\n",
    "\n",
    "#predict the target variable on the test set\n",
    "X_test = df_test.drop(\"income\", axis=1)\n",
    "y_test = df_test[\"income\"]\n",
    "\n",
    "\n",
    "# Predict the target variable on the test set\n",
    "y_pred = gam.predict(X_test)\n",
    "y_pred_proba = gam.predict_proba(X_val)\n",
    "\n",
    "# Calculate the performance metrics including F1 score, ROC_AUC and confusion matrix\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Precision-Recall Display\n",
    "PrecisionRecallDisplay.from_predictions(y_test, gam.predict_proba(X_test), ax=axs[0])\n",
    "axs[0].set_title('Precision-Recall Curve - Test Set')\n",
    "\n",
    "# ROC Curve Display\n",
    "RocCurveDisplay.from_predictions(y_test, gam.predict_proba(X_test), ax=axs[1])\n",
    "axs[1].set_title('ROC Curve - Test Set')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Classification Report - Test Set:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the 4 most significant features, plot the shape functions of the GAM model in a 2x2 subplot. \n",
    "# rescale the features to their original scale using the saved scaling parameters\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Get the p-values of the features in the GAM model\n",
    "p_values = gam.statistics_[\"p_values\"]\n",
    "\n",
    "# Get the indices of the 4 most significant features\n",
    "top_features_indices = np.argsort(np.abs(p_values))[:4]\n",
    "\n",
    "# Get the feature names of the 4 most significant features\n",
    "top_features = [feature_names[int(i)] for i in top_features_indices]\n",
    "\n",
    "# Rescale the features to their original scale\n",
    "X_val_rescaled = X_val.copy()\n",
    "X_val_rescaled[numerical_features] = X_val_rescaled[numerical_features] * np.sqrt(scaler_var) + scaler_mean\n",
    "\n",
    "# get the mapping for the categorical variables\n",
    "label_encoders = joblib.load(\"label_encoders.pkl\")\n",
    "\n",
    "#print the mapping for the categorical variables\n",
    "# for col in categorical_features:\n",
    "#     print(f\"Mapping for {col}: {dict(enumerate(label_encoders[col].classes_))}\")\n",
    "\n",
    "# Plot the shape functions of the GAM model for the 4 most significant features\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    term_index = int(top_features_indices[i])\n",
    "    XX = gam.generate_X_grid(term=term_index)\n",
    "    XX_rescaled = XX.copy()\n",
    "    if feature_names[term_index] in numerical_features:\n",
    "        XX_rescaled[:, term_index] = XX[:, term_index] * np.sqrt(scaler_var[numerical_features.index(feature_names[term_index])]) + scaler_mean[numerical_features.index(feature_names[term_index])]\n",
    "    pdep, confi = gam.partial_dependence(term=term_index, width=.95)\n",
    "    ax.plot(XX_rescaled[:, term_index], pdep)\n",
    "    ax.plot(XX_rescaled[:, term_index], confi, c='r', ls='--')\n",
    "    ax.set_title(f\"Feature: {top_features[i]}\")\n",
    "    if feature_names[term_index] in categorical_features:\n",
    "        ax.set_xticks(range(len(label_encoders[feature_names[term_index]].classes_)))\n",
    "        ax.set_xticklabels(label_encoders[feature_names[term_index]].classes_, rotation=90)\n",
    "    ax.set_xlabel(top_features[i])\n",
    "    ax.set_ylabel(\"Partial Dependence\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of GAM model.\n",
    "\n",
    "1. **Feature: Age**\n",
    "    -\tStrong positive effect from ~20 to 50 years: The probability of earning >$50K increases sharply as age increases.\n",
    "    -\tPeak impact at ~50-55 years: The effect stabilizes at this range.\n",
    "    -\tDeclining effect after 60-70 years: Likely due to retirement or reduced working hours.\n",
    "\n",
    "2. **Feature: Education-Num (Years of education)**\n",
    "\n",
    "    - The probability of earning >$50K increases almost linearly with education.\n",
    "    - A more steep increase is visible at 12+ years of education (college degree level).\n",
    "    - After 16 years, the impact still rises, but at a slightly diminishing rate.\n",
    "    - Advanced degrees (Master’s, PhD) still improve income, but the return is smaller compared to high school vs. Bachelor’s.\n",
    "\n",
    "3. **Feature: Marital Status**\n",
    "\n",
    "    - Married individuals (esp. Married-civ-spouse) have the highest probability of earning >$50K.\n",
    "    - Divorced & separated individuals show a higher probability than never-married but lower than married individuals.\n",
    "    - Never-married group has the lowest probability.\n",
    "\n",
    "4. **Feature: Ocupation**\n",
    "\n",
    "    - “Exec-managerial”, “Prof-specialty” (Professional jobs) → High positive effect\n",
    "    - “Sales” & “Tech-support” → Moderate positive effect.\n",
    "    - “Handlers-cleaners”, “Priv-house-serv”, “Other-service” → Negative or low effect.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
