{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Local Interpretable Model Agnostic Explanations (LIME)**\n",
    "\n",
    "This notebook demonstrates the use of LIME - a post-hoc methods for explainability.\n",
    "\n",
    "- In this technique, simpler explainable surrogate models are trained to approximate the predictions of an underlying complex black box model. \n",
    "\n",
    "- It provides local explanations, meaning it explains why a model made a specific prediction for an individual instance (e.g., one patient, one image, one data point).\n",
    "- LIME tests what happens to black box model predictions when pertubations are applied to the inputs. \n",
    "- STEPS:\n",
    "    1. Take the Instance to Explain\n",
    "    2. Perturb the features slightly for this instance\n",
    "        - tabular data: modifying feature valus slightly\n",
    "        - image data: masking parts of an image\n",
    "    3. Predict with complex black box model\n",
    "    4. Assign weights to the perturbed instances based on how similar they are to original instance. Closer instances are weighted higher\n",
    "    5. Train a simpler model to the predictions of the black box model to the perturbed instances\n",
    "    6. From simpler model, extract feature importance scores for the original instance. Scores indicate which feature contributed most to the prediction.\n",
    "    7. Visualize explanations.\n",
    "\n",
    "\n",
    "The original paper describing this approach is https://arxiv.org/pdf/1602.04938\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import lime.lime_tabular\n",
    "np.random.seed(1)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import pickle \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import Gas Turbine Sensor Dataset**\n",
    "\n",
    "The dataset contains 36733 instances of 11 sensor measures aggregated over one hour, from a gas turbine located in Turkey for the purpose of studying flue gas emissions, namely CO and NOx. Link to dataset is https://archive.ics.uci.edu/dataset/551/gas+turbine+co+and+nox+emission+data+set\n",
    "\n",
    "The dataset has 11 sensor measures aggregated over one hour. \n",
    "| Variable Name | Role    | Type       | Description                                    | Units  | Min    | Max    | Mean   | Missing Values |\n",
    "|--------------|--------|------------|------------------------------------------------|--------|--------|--------|--------|----------------|\n",
    "| year         | Feature | Integer    | Year of observation                           | -      | -      | -      | -      | no             |\n",
    "| AT          | Feature | Continuous | Ambient temperature                           | °C     | –6.23  | 37.10  | 17.71  | no             |\n",
    "| AP          | Feature | Continuous | Ambient pressure                              | mbar   | 985.85 | 1036.56| 1013.07| no             |\n",
    "| AH          | Feature | Continuous | Ambient humidity                              | %      | 24.08  | 100.20 | 77.87  | no             |\n",
    "| AFDP        | Feature | Continuous | Air filter difference pressure                | mbar   | 2.09   | 7.61   | 3.93   | no             |\n",
    "| GTEP        | Feature | Continuous | Gas turbine exhaust pressure                  | mbar   | 17.70  | 40.72  | 25.56  | no             |\n",
    "| TIT         | Feature | Continuous | Turbine inlet temperature                     | °C     | 1000.85| 1100.89| 1081.43| no             |\n",
    "| TAT         | Feature | Continuous | Turbine after temperature                     | °C     | 511.04 | 550.61 | 546.16 | no             |\n",
    "| TEY         | Feature | Continuous | Turbine energy yield                          | MWH    | 100.02 | 179.50 | 133.51 | no             |\n",
    "| CDP         | Feature | Continuous | Compressor discharge pressure                 | mbar   | 9.85   | 15.16  | 12.06  | no             |\n",
    "| CO          | Feature | Continuous | Carbon monoxide concentration                 | mg/m³  | 0.00   | 44.10  | 2.37   | no             |\n",
    "| NOx         | Feature | Continuous | Nitrogen oxides concentration                 | mg/m³  | 25.90  | 119.91 | 65.29  | no             |\n",
    "\n",
    "\n",
    "\n",
    "Let us treat this as a regression problem and use all sensor data to predict turbine energy yield. We first will concatenate data from 4 years (2011-2014) and use data from 2015 to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = '/Users/Dhaneshr/code/interpretability-bootcamp/reference_implementations/Post-hoc/datasets/gas+turbine+co+and+nox+emission+data+set'\n",
    "\n",
    "# Load the CSV files\n",
    "files_to_load = ['gt_2011.csv', 'gt_2012.csv', 'gt_2013.csv', 'gt_2014.csv']\n",
    "dataframes = [pd.read_csv(os.path.join(folder_path, file)) for file in files_to_load]\n",
    "\n",
    "\n",
    "# Concatenate the dataframes\n",
    "train_val_data = pd.concat(dataframes, ignore_index=True)\n",
    "display(train_val_data.head())\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(os.path.join(folder_path, 'gt_2015.csv'))\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_val_data = pd.DataFrame(scaler.fit_transform(train_val_data), columns=train_val_data.columns)\n",
    "test_data = pd.DataFrame(scaler.transform(test_data), columns=test_data.columns)\n",
    "\n",
    "\n",
    "# Split the training and validation data\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.2, random_state=42)\n",
    "\n",
    "#use column TEY as target\n",
    "target = 'TEY'\n",
    "X_train = train_data.drop(target, axis=1)\n",
    "y_train = train_data[target]\n",
    "X_val = val_data.drop(target, axis=1)\n",
    "y_val = val_data[target]\n",
    "X_test = test_data.drop(target, axis=1)\n",
    "y_test = test_data[target]\n",
    "\n",
    "\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(f'Training data shape: {train_data.shape}')\n",
    "print(f'Validation data shape: {val_data.shape}')\n",
    "print(f'Test data shape: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Set up Regression Model**\n",
    "\n",
    "- To illustrate this example, we shall use a Gradient Boosting Regression Model. \n",
    "- Let's use grid search to find the best hyperparameters for this problem.  For simplicity, we will just pick 3 sets of hyperparameters to save time. \n",
    "- after grid search, we will save the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [3, 4, 6],\n",
    "    'learning_rate': [0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters found: {best_params}')\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "gbr = GradientBoostingRegressor(**best_params, random_state=1)\n",
    "\n",
    "# Train the model\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_val_pred = gbr.predict(X_val)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(gbr, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "print(f'Mean Squared Error on validation data: {mse}')\n",
    "\n",
    "ytest_pred = gbr.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, ytest_pred)\n",
    "print(f'Mean Squared Error on test data: {mse_test}')\n",
    "r2 = r2_score(y_test, ytest_pred)\n",
    "print(f'R2 score on test data: {r2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.LIME local Explanations**\n",
    "\n",
    "Let's use LIME to provide insights into its predictions and correlate that with domain knowledge about gas turbine generators.\n",
    "Here we will load the trained model, and use LIME's `explain_instance()` function to predict the 12th sample from the test set \n",
    "and plot attributions related to 6 different sensor readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's load the saved model and use LIME to explain the predictions\n",
    "filename = 'finalized_model.sav'\n",
    "gbr = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, mode='regression', feature_names=X_train.columns.tolist(), discretize_continuous=True)\n",
    "\n",
    "# Pick the 12th instance from the test data\n",
    "i = 12\n",
    "exp = explainer.explain_instance(X_test.values[i], gbr.predict, num_features=6) \n",
    "display(X_test.iloc[i]) \n",
    "exp.show_in_notebook()\n",
    "# fig = exp.as_pyplot_figure(label=1)\n",
    "# fig.savefig('lime_oi.png')\n",
    "# print(exp.as_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here iwe note that for the model's prediction on that 12th instance, the low ambient temperature and \n",
    "| Feature Condition         | Contribution | Interpretation |\n",
    "|---------------------------|-------------|----------------|\n",
    "| **AT <= -0.81** (Positive)   | **+0.239**  | A very **low Ambient Temperature (AT)** significantly **increases** the model’s predicted output. This suggests that turbine performance improves in colder conditions. |\n",
    "| **-0.58 < TIT <= 0.27** (Negative)  | **-0.100** | A **moderate Turbine Inlet Temperature (TIT)** **lowers** the predicted output, which may indicate that higher temperatures are needed for optimal turbine efficiency. |\n",
    "| **-0.58 < CDP <= -0.08** (Negative)  | **-0.099** | A **low Compressor Discharge Pressure (CDP)** **reduces** the predicted output, possibly due to lower compression efficiency in the turbine system. |\n",
    "| **-0.56 < GTEP <= -0.09** (Negative) | **-0.063** | A **low Gas Turbine Exhaust Pressure (GTEP)** is associated with a **decrease** in the predicted value, likely due to inefficiencies in exhaust flow. |\n",
    "| **AP > 0.62** (Negative)   | **-0.060**  | A **higher Ambient Pressure (AP)** has a **negative effect** on the predicted outcome, suggesting that increased pressure might create inefficiencies in the turbine system. |\n",
    "| **AFDP <= -0.66** (Positive) | **+0.046**  | A **low Air Filter Difference Pressure (AFDP)** slightly **increases** the predicted value, indicating that less resistance in air intake improves efficiency. |\n",
    "\n",
    "The turbine’s energy yield (TEY) depends on environmental conditions and machine parameters: \n",
    "\n",
    "- Cooler temperatures (low AT) improve efficiency, which aligns with real-world turbine behavior (lower temperatures improve air density and combustion).\n",
    "- Compressor and turbine inlet pressures impact performance, and a low compressor discharge pressure (CDP) reduces efficiency.\n",
    "- Turbine exhaust pressure (GTEP) also influences yield, as inefficient exhaust flow can reduce power generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extracting Global Explanations**\n",
    "\n",
    "LIME provide inherently local explanations. How can we then obtain global interpretation of the model's decision?\n",
    "One way is to use a **Global Surrogate** : the idea here is to use a simple intepretable model to explain the predictions of the more complex model. \n",
    "In this example, let's use a simple decision tree..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Train a simple Decision Tree as a surrogate model\n",
    "surrogate_model = DecisionTreeRegressor(max_depth=3)\n",
    "surrogate_model.fit(X_train, gbr.predict(X_train))\n",
    "\n",
    "# Visualize the tree to interpret global importance\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(surrogate_model, feature_names=X_train.columns, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot of the decision tree, we can see that CDP (Compressor Discharge Pressure) seems to be the most important feature, followed by the AT (Ambient Temperature). \n",
    "\n",
    "| Feature Split | Impact on TEY | Explanation |\n",
    "|--------------|--------------|-------------|\n",
    "| **CDP ≤ 0.473** | Decreases TEY | Low compressor discharge pressure reduces turbine efficiency. |\n",
    "| **CDP > 1.461** | Increases TEY | High CDP improves performance. |\n",
    "| **AT ≤ -0.925** | Increases TEY | Low ambient temperature enhances turbine efficiency. |\n",
    "| **CDP ≤ -1.111** | Strongly Decreases TEY | Extremely low CDP causes major efficiency loss. |\n",
    "\n",
    "\n",
    "1.\tCDP (Compressor Discharge Pressure) is the Most Important Feature:\n",
    "- It is used for the first split, meaning it strongly influences TEY.\n",
    "- Lower CDP results in lower TEY (left side of the tree).\n",
    "- Higher CDP leads to higher TEY (right side of the tree).\n",
    "2.\tAmbient Temperature (AT) is Also Important:\n",
    "- The model splits on AT ≤ -0.925, showing that colder temperatures increase TEY.\n",
    "- This aligns with the physics of turbines, where cooler air improves efficiency.\n",
    "3.\tVariance Decreases as We Move Down the Tree:\n",
    "- The squared error (uncertainty) decreases at deeper nodes.\n",
    "- This means the model is making more confident predictions as it refines splits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
